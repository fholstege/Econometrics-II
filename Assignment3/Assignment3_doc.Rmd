---
title: "Econometrics II - Assignment 3"
author: Floris Holstege, Stanislav Avdeev
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(plm, 
               Formula,
               car,
               clubSandwich,
               lmtest,
               foreign,
               tidyverse,
               xtable,
               stargazer)

```

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# create df with treatment effects
dfTreatment <- data.frame(N_treated = c(100, 75, 25), N_control = c(100, 25, 75), Avg_Outcome_treated = c(9, 13,10), Avg_Outcome_control = c(7,8,9))
row.names(dfTreatment) <- c("Purple", "Blue", "Green")

# average treatment effect per group
ATE_group <- dfTreatment$Avg_Outcome_treated - dfTreatment$Avg_Outcome_control

ATE_treated <- sum(dfTreatment$N_treated/sum(dfTreatment$N_treated) * dfTreatment$Avg_Outcome_treated)
ATE_control <- sum(dfTreatment$N_control/sum(dfTreatment$N_control) * dfTreatment$Avg_Outcome_control)
ATE_treated - ATE_control


```


# Problem 1

Before we answer this question, let's lay out some basic terms that are useful for understanding our subsequent. We define the average treatment effect (ATE) as:

\begin{align*}
    ATE = \mathbb{E}(\delta) = \mathbb{E}(Y^*_1 - Y^*_0) = \mathbb{E}(Y^*_1) - \mathbb{E}(Y^*_0). 
\end{align*}

Where $Y^*_1$ is the latent variable of interest for the group that received treatment. For this group, $D_i = 1$. $Y^*_0$ is the latent variable of interest for the group that did not receive treatment. We only observe $\mathbb{E}(Y^*_1)$, since our control group does not receive the treatment. Given we only observe the effect when $D_i = 1$, we define the average treatment effect of the treated as (ATET).

\begin{align*}
    ATET = \mathbb{E}(\delta | D = 1) = \mathbb{E}(Y^*_1 - Y^*_0 | D = 1) = \mathbb{E}(Y^*_1 | D = 1) - \mathbb{E}(Y^*_0 | D = 1). 
\end{align*}

If the treatment is assigned randomly, then $(Y^*_{1i}, Y^*_{0i}) \perp D_i$. In this case, ATE = ATET, since there is no significant difference between the characteristics of the treatment and control group. For this question, we assume that the treatment has been randomly assigned, and thus that ATE = ATET. 

\begin{itemize}
  \item Average treatment per group: $ATE_{purple} = 9 - 7 = 2$ , $ATE_{blue} = 13-8 = 5$, $ATE_{green} = 10-9=1$
  \item Average treatment for the full population: $ATE = \mathbb{E}(Y^*_1) - \mathbb{E}(Y^*_0) = 10.625 - 7.875 = 2.75$
  \item Average treatment for the treated: $ATE = ATET = 2.75$
\end{itemize}

\pagebreak 

# Problem 2

## I)

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}


dfBonus <- read.dta("../Data/bonus.dta")

dfBonus$category <- ifelse(dfBonus$bonus500 == 1, "Low-reward", ifelse(dfBonus$bonus1500 == 1, "High-reward", "Control"))

dfSummaryStats <- dfBonus %>% 
                              group_by(category) %>% 
                              summarise(perc_passed_year1 = sum(pass)/ n(), 
                                        avg_myeduc = mean(myeduc),
                                        avg_fyeduc = mean(fyeduc), 
                                        avg_p0 = mean(p0),
                                        math = mean(math[!is.na(math)]),
                                        perc_job = sum(job[!is.na(job)])/n(),
                                        avg_effort = mean(effort[!is.na(effort)]))


xtable(dfSummaryStats, digits = 3)
t.test(dfBonus[dfBonus$category == "Control",]$effort, dfBonus[dfBonus$category == "High-reward",]$effort)


# COMMENT FLO: How to deal with NA's? I think we need to just mention this as a limitation in checking how different they are. Maybe even quantify how much bias this could result in. 

# Stas: we should not include NA's when we calculate the mean for employment

# There are some differences in average subjective assesment of the student about his probability to succeed. Maybe we can use t.test to check whether the difference is statisticallly significant

```

Before discussing descriptive statistics, we need to make several assumptions. First, we assume all variables except for total number of credits, an employment status, and an amount of efforts were measured before the randomization. Thus, we can use variables that were measured prior to the randomization to check whether group characteristics are balanced or not.

Table \ref{stats} suggests that there are no differences for an average number of years of education for both parents and for high-school math score between the control and two treatment groups. We quickly checked if the difference between the effort for students in the control group and the high-reward group was statistically significant - its not (student's t-test returns a p-value of 0.45), and so we feel comfortable in proceeding. 

We are going to look at the following outcomes: a) percentage of students who completed all first-year courses and b) total number of credit points after one and three years of studies. 

\begin{table}[ht]
  \caption{Mean values for observed characteristics by groups} 
  \label{stats} 
\centering
\begin{tabular}{rlrrrrrrr}
  \hline
 & Group & perc\_passed\_year1 & avg\_myeduc & avg\_fyeduc & avg\_p0 & math & perc\_job & avg\_effort \\ 
  \hline
1 & Control & 19.5\% & 12.293 & 13.378 & 0.553 & 5.476 & 69.5\% & 19.549 \\ 
  2 & High-reward & 24.1\% & 12.590 & 13.422 & 0.573 & 5.388 & 74.7\% & 18.303 \\ 
  3 & Low-reward & 20.2\% & 12.119 & 13.524 & 0.530 & 5.386 & 81.0\% & 18.477 \\ 
   \hline
\end{tabular}
\end{table}





## II)

As we run a randomized experiment, the orthogonality assumtion of the OLS regression is held. Thus, we are using conventional standard errors as they are more efficient. The basic model is the following:

\begin{align*}
    COMPLETE_{i} = \alpha_0 +\alpha_1 HIGH\_REWARD_{i} + \alpha_2 LOW\_REWARD_{i} + \epsilon_{i}
\end{align*}

We also include the following regressors to estimate an extended model:

\begin{align*}
    COMPLETE_{i} = \alpha_0 +\alpha_1 HIGH\_REWARD_{i} + \alpha_2 LOW\_REWARD_{i} + \alpha_3 EDU\_FATHER_{i} + \alpha_4 MATH_{i} + \\ \alpha_5 ASSESSMENT_{i} + \epsilon_{i}
\end{align*}

Columns (1) and (2) of Table \ref{models} below show that there are no statistically significant differences between control and two treatment groups in the probabability of completing all first-year courses. The results do not change when we include the additional control variables. Based on the results we conclude that financial incentives do not affect the probability of successfully finishing the first year of the studies. 

\begin{table}[!htbp] \centering 
  \caption{The effect of a financial reward on completing all first-year courses} 
  \label{models} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{3}{c}{pass} \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\hline \\[-1.8ex] 
 categoryHigh-reward & 0.046 & 0.048 & 0.056 \\ 
  & (0.064) & (0.058) & (0.059) \\ 
 categoryLow-reward & 0.007 & 0.015 & 0.023 \\ 
  & (0.064) & (0.057) & (0.058) \\ 
 math &  & 0.119$^{***}$ & 0.124$^{***}$ \\ 
  &  & (0.018) & (0.018) \\ 
 fyeduc &  & $-$0.001 & 0.0003 \\ 
  &  & (0.007) & (0.007) \\ 
 p0 &  & 0.249$^{***}$ & 0.170$^{*}$ \\ 
  &  & (0.095) & (0.098) \\ 
 effort &  &  & 0.008$^{***}$ \\ 
  &  &  & (0.002) \\ 
 job &  &  & $-$0.062 \\ 
  &  &  & (0.060) \\ 
 Constant & 0.195$^{***}$ & $-$0.576$^{***}$ & $-$0.678$^{***}$ \\ 
  & (0.045) & (0.128) & (0.144) \\ 
\hline \\[-1.8ex] 
Observations & 249 & 245 & 230 \\ 
Adjusted R$^{2}$ & $-$0.006 & 0.192 & 0.240 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

## III)

In the third model we are going to include variables that were measured after the randomization process: dummy variable on whether the student had  a job while studying and the amount of study effort. In line with previous results, column (3) of Table \ref{models} yields an insignificant effect of getting financial rewards on completing all first-year courses. However, an employment status while studying and the amount of study effort are potentially endogenous variables as they can be correlated with a treatment status of a student. One might argue that a possibility to earn 500 or 1500 guilders could have influenced a student's decision to enter the labour market while studying. Moreover, it could have influenced the amount of effort a student dedicates to their studies, as more efforts mean higher probability of getting financial rewards. Thus, these two variables are "bad" controls that one should not include in the model.

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# define the three LPM models
LPM_simple <- lm(pass ~ category, data=dfBonus)
LPM_addedRegressors <- lm(pass ~ category + math + fyeduc + p0, data=dfBonus)
LPM_allRegressors <- lm(pass ~ category + math + fyeduc + p0 + effort + job, data=dfBonus)

# check adjusted R2 and coefficients
stargazer(LPM_simple, LPM_addedRegressors, LPM_allRegressors,
          keep.stat=c("n","adj.rsq"))

# COMMENT FLO: my preferred model is the one with the most variables - 1) All of these capture variables that might influence the relationship, 2) adding these variables increases the adjusted R2, and 3) there is no clear indication of multicollinearity (see correlation matrix and VIF below)
mX <- dfBonus %>% select(math, fyeduc, p0, effort, job) %>% as.matrix() %>% na.omit()
cor(mX)
vif(LPM_allRegressors)



```


## IV)

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
LPM_drop <- lm(dropout ~ category, data=dfBonus)
LPM_drop_all <- lm(dropout ~ category + math + fyeduc + p0, data=dfBonus)
LOGIT_drop <- glm(dropout ~ category, data=dfBonus, 
             family = binomial(link = "logit"), x = TRUE)
LOGIT_drop_all <- glm(dropout ~ category + math + fyeduc + p0, data=dfBonus, 
             family = binomial(link = "logit"), x = TRUE)
margins_1 <- maBina(LOGIT_drop, x.mean = FALSE)
margins_2 <- maBina(LOGIT_drop_all, x.mean = FALSE)


stargazer(LPM_drop, LPM_drop_all, margins_1, margins_2,
          keep.stat=c("n","adj.rsq"))

LM_pointsYear1 <- lm(stp2001 ~ category, data=dfBonus)
LM_pointsYear1_2 <- lm(stp2001 ~ category + math + fyeduc + p0, data=dfBonus)
LM_pointsYear3 <- lm(stp2004 ~ category, data=dfBonus)
LM_pointsYear3_2 <- lm(stp2004 ~ category + math + fyeduc + p0, data=dfBonus)

stargazer(LM_pointsYear1, LM_pointsYear1_2, LM_pointsYear3, LM_pointsYear3_2,
          keep.stat=c("n","adj.rsq"))
```

We are going to use a linear probability and a logit models with only a financial dummy reward and other exogenous control variables included. We want to use a logit model to check whether there are going to be big differencess between the two models. As we use data from a randomized experiment we can use a model with only a financial dummy variable included due to the nature of randomization. If an RCT was successfully conducted, the treatment assignment is statistically independent of potential outcomes, i.e. $(Y^*_{0i}, Y^*_{1i}) \perp D_i$. However, controlling for other regressors reduces standard errors around treatment effect. Besides, if the estimated size of the treatment effect differs with and without including covariates, then one should about the additional covariates or the initial randomization process.

Columns (1) and (2) of Table \ref{drop} show the results of the linear probability model while columns (3) and (4) yield the marginal effects of a logit model. The results indicate that there are no statististically significant effects of a financial reward on the probability to drop out regardless of the amoung of this reward. Including control variables and using a logit model instead of a linear probability model do not change the results. The results are robust to different specifications and models used.

\begin{table}[!htbp] \centering 
  \caption{The effect of a financial reward on the probability to drop out} 
  \label{drop} 
\begin{tabular}{@{\extracolsep{5pt}}lcccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{dropout} \\ 
\\[-1.8ex] & \multicolumn{2}{c}{\textit{OLS}} & \multicolumn{2}{c}{\textit{binary model}} \\ 
 & \multicolumn{2}{c}{\textit{}} & \multicolumn{2}{c}{\textit{(marginal effect)}} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4)\\ 
\hline \\[-1.8ex] 
 categoryHigh-reward & $-$0.053 & $-$0.053 & $-$0.052 & $-$0.053 \\ 
  & (0.075) & (0.073) & (0.073) & (0.076) \\ 
 categoryLow-reward & $-$0.057 & $-$0.069 & $-$0.056 & $-$0.070 \\ 
  & (0.075) & (0.073) & (0.073) & (0.075) \\ 
 math &  & $-$0.075$^{***}$ &  & $-$0.076$^{***}$ \\ 
  &  & (0.023) &  & (0.024) \\ 
 fyeduc &  & 0.007 &  & 0.006 \\ 
  &  & (0.009) &  & (0.009) \\ 
 p0 &  & $-$0.328$^{***}$ &  & $-$0.322$^{***}$ \\ 
  &  & (0.121) &  & (0.120) \\ 
 Constant & 0.402$^{***}$ & 0.903$^{***}$ & $-$0.091$^{*}$ & 0.418$^{**}$ \\ 
  & (0.053) & (0.163) & (0.050) & (0.169) \\ 
\hline \\[-1.8ex] 
Observations & 249 & 245 & 249 & 245 \\ 
Adjusted R$^{2}$ & $-$0.005 & 0.068 &  &  \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

Columns (1) and (2) of Table \ref{points} show the effects of a financial reward on the number of credits a student earned during the first year while columns (3) and (4) show the results after three years. The effect of a financial reward is zero and not statistically significant either for both outcomes.

\begin{table}[!htbp] \centering 
  \caption{The effect of a financial reward on the number of credits} 
  \label{points} 
\begin{tabular}{@{\extracolsep{5pt}}lcccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{2}{c}{stp2001} & \multicolumn{2}{c}{stp2004} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4)\\ 
\hline \\[-1.8ex] 
 categoryHigh-reward & $-$0.437 & $-$0.262 & $-$1.185 & $-$0.673 \\ 
  & (3.460) & (3.038) & (9.809) & (9.080) \\ 
 categoryLow-reward & $-$1.531 & $-$0.699 & $-$2.452 & $-$1.306 \\ 
  & (3.450) & (3.010) & (9.780) & (8.998) \\ 
 math &  & 6.454$^{***}$ &  & 15.127$^{***}$ \\ 
  &  & (0.960) &  & (2.869) \\ 
 fyeduc &  & $-$0.513 &  & $-$1.469 \\ 
  &  & (0.374) &  & (1.118) \\ 
 p0 &  & 21.632$^{***}$ &  & 44.689$^{***}$ \\ 
  &  & (5.026) &  & (15.023) \\ 
 Constant & 33.152$^{***}$ & $-$7.281 & 84.274$^{***}$ & $-$3.596 \\ 
  & (2.454) & (6.745) & (6.957) & (20.162) \\ 
\hline \\[-1.8ex] 
Observations & 249 & 245 & 249 & 245 \\ 
Adjusted R$^{2}$ & $-$0.007 & 0.235 & $-$0.008 & 0.142 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

\newpage

## V)

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
lm_pass_high <- lm(pass ~ bonus1500 + math + fyeduc + p0, dfBonus[dfBonus$category != "Low-reward",])
lm_pass_low <- lm(pass ~ bonus500 + math + fyeduc + p0, dfBonus[dfBonus$category != "High-reward",])

get_MDE <- function(lm_model, group, alpha, power, dependent=NULL){
  # Get the data
  dfModel <- lm_model$model 
  
  # N of coefficients
  n_coef <- length(lm_model$coefficients)
  
  # N of observations
  n <- nrow(dfModel)
  
  # N of treated
  nTreatment <- sum(dfModel[,group])
  
  # Get the share of treated
  p <- nTreatment/n
  
  # Get t-value
  t_alpha <- qt(1-alpha/2, n - n_coef)
  t_q <- qt(1-power, n - n_coef)
  
  # get variance of residuals
  # Floris: Stanislav, here I program the option to use the passing rate of the population as the variance
  if(is.null(dependent)){
    
    sigma2 <- var(lm_model$residuals)
    
  }else if (is.character(dependent)){
    
    vY <- dfModel[,dependent]
    perc_pass <- sum(vY)/n
    sigma2 <- perc_pass* (1- perc_pass)
    
  }
  
  # get the MDE
  MDE <- (t_alpha - t_q) * sqrt(1/(p*(1-p))) * sqrt(sigma2/n)
  
  return(MDE)
}

nrow(lm_pass_low$model)
get_MDE(lm_pass_high, "bonus1500", 0.05, 0.8, "pass")
get_MDE(lm_pass_low, "bonus500", 0.05, 0.8, "pass")

```

To estimate the minimum detectable effect size of the financial reward on the probability to complete all first-year courses, we are using the following formula:

\begin{align*}
    MDE = (t_{1-\alpha/2} - t_{1-q})\sqrt{\frac{1}{p(1-p)}}\sqrt{\frac{\sigma^2}{n}}
\end{align*}

where $\alpha$ - significance level; $q$ - power; $p$ - a share of treated students; $\sigma^2$ - variance of OLS residuals. We specify these parameters as follows: 
\begin{itemize}
  \item We use conventional values for $alpha = 0.05$ and $q = 0.8$. 
  \item For the t-values, we pick the degrees of freedom as $n - k$, with $n$ the number of observations and $k$ the number of coefficients. 
  \item For $\sigma^2$: the residuals should follow the distribution of a bernouli value (since the only possible values are 1 and 0). The variance of a bernouli variable is $P$(1-$P$), with $P$ as the probability of the variable taking the value of 1. We estimate $P$ as the percentage of students in the population (both control and treated) that passed the first year. 
\end{itemize}

Our sample of students consists of three groups; control, low-reward, and high-reward. We do not want to put all of these together in the same MDE calculation. The resulting MDE would be the MDE as if both groups received the same treatment - but that is not the case. 

One could argue that they are similar for the purpose of calculating the MDE, using the monotinicity assumption that those who would be motivated by the low reward should also be motivated by the high reward. But we do not want to make this assumption.As such, we conduct the MDE calculation separately for two models; one with the control group and the low-reward, and one with the control group and the high-reward. In both cases, we used the model as specified in question II) with the control variables. 

\begin{table}[ht]
\centering
\begin{tabular}{ccc}
  \hline
  Test & sample size & MDE \\ 
  \hline
Control vs Low reward & 165 & 19.3\% \\
 Control vs High reward & 162 & 20.5\% \\
   \hline
\end{tabular}
\end{table}




## VI)



```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
get_size <- function(lm_model, group, alpha, power, MDE, dependent =NULL){
  # Get the data
  dfModel <- lm_model$model 
  
  # N of coefficients
  n_coef <- length(lm_model$coefficients)
  
  # N of observations
  n <- nrow(dfModel)
  
  # N of treated
  nTreatment <- sum(dfModel[,group])
  
  # Get the share of treated
  p <- nTreatment/n
  
  # Get t-value
  t_alpha <- 1.96
  t_q <- -0.84
  
  # get variance of residuals
  # Floris: Stanislav, here I program the option to use the passing rate of the population as the variance
  if(is.null(dependent)){
    
    sigma2 <- var(lm_model$residuals)
    
  }else if (is.character(dependent)){
    
    vY <- dfModel[,dependent]
    perc_pass <- sum(vY)/n
    sigma2 <- perc_pass* (1- perc_pass)
    
  }
  

  # get the MDE
  size <- (((t_alpha - t_q)/MDE)^2) * sigma2/(p*(1-p))
  size <- round(size, 0)
  return(size)
}



get_size(lm_points_high, "bonus1500", 0.05, 0.8, 0.1, "pass")
get_size(lm_points_low, "bonus500", 0.05, 0.8, 0.1, "pass")
```

For an MDE of 10\%, we need to calculate the sample size according to this formula:

\begin{align*}
    n = \left(\frac{t_{1-\alpha/2} - t_{1-q}}{MDE}\right)^2\frac{\sigma^2}{p(1-p)}
\end{align*}

We make the same assumptions as specified in the previous question, with one exception - since we do not know the N to find the degrees of freedom for the t-values, we presume a very large n, in which the t-distribution converges to a normal distribution. 


\begin{table}[ht]
\centering
\begin{tabular}{cccc}
  \hline
 Test & sample size & Desired MDE & Required sample size\\ 
  \hline
Control vs Low reward & 165 & 10\% & 531 \\
 Control vs High reward & 162 & 10\% & 490 \\
   \hline
\end{tabular}
\end{table}


