---
title: "Econometrics II - Assignment 2"
author: Floris Holstege, Stanislav Avdeev
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Packages required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(rio,
               stargazer,
               plm,
               parameters)

# load dataset
df <- import("Data/NLSY2000RC_V2.csv")
df$EARNINGS <- log(df$EARNINGS)

colnames(df) <- c("TIME", "ID", "AGE", "Schooling", "Ethnicity", "Test_score", "URBAN", "REGNE", "REGNC", "REGW", "REGS", "EARNINGS", "AGESQ")

# covariance matrix with White SE
#vcovWhite <- function(x) vcovHC(x, method="white1")
# should we just delete it?

```

## 1

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# Run poooled OLS including and excluding Test_score
lm_pooled_no_ab <- plm(EARNINGS ~ Schooling + Ethnicity + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df, model = "pooling", index = "ID")
lm_pooled_ab <- plm(EARNINGS ~ Schooling + Ethnicity + Test_score + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df, model = "pooling", index = "ID")

stargazer(lm_pooled_no_ab, lm_pooled_ab,
          keep.stat=c("n","adj.rsq"),
          se = list(vcovHC(lm_pooled_no_ab), vcovHC(lm_pooled_no_ab)),
          title="OLS pooled model with and without ability variable")

# and change variable names to decent ones
# add citation
```

As a base model we use the following model specification:

\begin{align*}
  Log(Earnings) = \alpha_0 +\alpha_1 Schooling_{it} + \alpha_2 AGE_{it} + \alpha_3 AGE^2_{it} + \alpha_4 ETHNICITY_{i} + \alpha_5 URBAN_{it} + \\ \alpha_6 REGNE_{it} + \alpha_7 REGNC{it} + \alpha_8 REGW_{it} 
\end{align*}

In order to check the impact of ability, we include a variable $ASVABC$ in the base model. Including an ability allows to account for an omitted variavble bias that most likely occurs in the base model without it.

The results of the base model with and without ability variable are as follows. When we do not account for ability in the model specification, returns to one year of education are higher, i.e. returns to a year of education are 7\% and statistically significant at the 1\% level. When we inlude the ability variable, returns to one year of education become 4.8\% and remain significant at the 1\% level. The underlying reason for such a drop is that higher ability students tend to get more education, thus, they tend to get higher earnings. 

\begin{table}[!htbp] \centering 
  \caption{OLS pooled model with and without ability variable} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{EARNINGS} \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 Schooling & 0.070$^{***}$ & 0.048$^{***}$ \\ 
  & ($-$0.00004) & ($-$0.00004) \\ 
  & & \\ 
 Test\_score &  & 0.011$^{***}$ \\ 
  &  & ($-$0.00004) \\ 
  & & \\ 
 Ethnicity & $-$0.192$^{***}$ & $-$0.096$^{***}$ \\ 
  & ($-$0.00004) & ($-$0.0002) \\ 
  & & \\ 
 Constant & $-$0.079$^{***}$ & $-$0.386$^{***}$ \\ 
  & (0.004) & (0.004) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 40,043 & 40,043 \\ 
Adjusted R$^{2}$ & 0.292 & 0.313 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

From now on we are going to include the ability variable in all model specifications due to two reasons. First, we deal with an omitted variable bias problem (theoretical problem). Second, including this variable improves our model: adjusted R squared is slightlty higher. Moreover, for the sake of saving the space we don't report all coefficients in tables unless they are important for a specific question.

## 2

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# Include interaction between schooling and ethnicity & run two models for two races separately
lm_inter <- plm(EARNINGS ~ Schooling + Schooling*Ethnicity + Ethnicity + Test_score + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df, model = "pooling", index = "ID")
lm_white <- plm(EARNINGS ~ Schooling + Test_score + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df[df$Ethnicity == 0,], model = "pooling", index = "ID")
lm_black <- plm(EARNINGS ~ Schooling + Test_score + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df[df$Ethnicity == 1,], model = "pooling", index = "ID")
stargazer(lm_inter, lm_white, lm_black,
          keep.stat=c("n","adj.rsq"),
          se = list(vcovHC(lm_inter), vcovHC(lm_white), vcovHC(lm_black)),
          title="OLS pooled model with heterogeneous effects by ethnicity")
```

As we can see, there is a statistically significant difference between returns to education by ethnicity. Black workers tend to get higher returns.

\begin{table}[!htbp] \centering 
  \caption{OLS pooled model with heterogeneous effects by ethnicity} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{3}{c}{EARNINGS} \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\hline \\[-1.8ex] 
 Schooling & 0.046$^{***}$ & 0.046$^{***}$ & 0.061$^{***}$ \\ 
  & ($-$0.00001) & ($-$0.00000) & ($-$0.0004) \\ 
  & & & \\ 
 Ethnicity & $-$0.295$^{***}$ &  &  \\ 
  & ($-$0.001) &  &  \\ 
  & & & \\ 
 Test\_score & 0.011$^{***}$ & 0.010$^{***}$ & 0.014$^{***}$ \\ 
  & ($-$0.00002) & ($-$0.00002) & ($-$0.0001) \\ 
  & & & \\ 
 Schooling:Ethnicity & 0.016$^{***}$ &  &  \\ 
  & (0.00004) &  &  \\ 
  & & & \\ 
 Constant & $-$0.370$^{***}$ & $-$0.437$^{***}$ & 0.038 \\ 
  & (0.004) & (0.005) & (0.036) \\ 
  & & & \\ 
\hline \\[-1.8ex] 
Observations & 40,043 & 35,223 & 4,820 \\ 
Adjusted R$^{2}$ & 0.313 & 0.299 & 0.314 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

## 3 

Once we use a panel model with random effects, there seems to be no difference returns to education by ethnicity.

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# Run random effect models with heteregoneity
lm_inter_rand <- plm(EARNINGS ~ Schooling + Schooling*Ethnicity + Ethnicity + Test_score + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df, model = "random", index = "ID")

stargazer(lm_inter_rand,
          keep.stat=c("n","adj.rsq"),
          title="Panel regressions with random effects")
```

## 4

## 5

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# Run fixed effect models with heteregoneity
lm_inter_fix <- plm(EARNINGS ~ Schooling + Schooling*Ethnicity + Ethnicity + Test_score + AGE + AGESQ + URBAN + REGNE + REGNC + REGW,
                    data = df, model = "within", index = "ID")

stargazer(lm_inter_fix,
          keep.stat=c("n","adj.rsq"),
          se = list(vcovHC(lm_inter_fix),
          title="Panel regressions with fixed effects")
```

## 6 

To decide between fixed or random effects we run a Hausman test. Under the $H_0$,  $E[\eta_i|X_{i1}, . . . , X_{iT} ] = 0$, and the random effects and fixed effects model are both consistent. In this case, the random effects model is preferred, since its more efficient than the fixed effects model. However, if $H_A$ is true, and $E[\eta_i|X_{i1}, . . . , X_{iT} ] \neq 0$, then the fixed effects model is preffered, since its the only consistent one of the two. Performing the Hausman test on our random effect and fixed effect model, we reject the $H_0$ that both models are consistent, and thus the fixed effects model is preferred.  

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
phtest(lm_inter_fix, lm_inter_rand)

```

## 7

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}


# get average over time per worker
X_hat <- data.frame(aggregate(. ~ ID, dfWorkers, mean))

# add column names
colnames(X_hat)[c(-1)] <- paste(colnames(X_hat)[c(-1)], "AVG", sep = "_")

# add to individual variables in single dataframe
dfMundlak = merge(dfWorkers, X_hat, by = "ID")

# create mundlak model 
Mundlak_model <- pggls(EARNINGS ~  S + AGE + AGESQ + ETHBLACK + URBAN + REGNE + REGNC + REGW  + ASVABC + S_AVG + AGE_AVG + AGESQ_AVG + ETHBLACK_AVG + URBAN_AVG + REGNE_AVG + REGNC_AVG + REGW_AVG, data=dfMundlak, model="random",index = c("ID"))
summary(Mundlak_model)

wald.test(b = coef(Mundlak_model), Sigma = vcov(Mundlak_model), Terms = 10:17)

```

## 8

## 9

```{r include=FALSE, eval=FALSE, echo=FALSE, results='hide'}
# Apply Verbeek and Nijman test
freq <- table(df$ID) 
freq <- as.data.frame(freq)
  
freq$Freq <- ifelse(freq$Freq > 4, freq$Freq, NA)
freq <- na.omit(freq)
```

