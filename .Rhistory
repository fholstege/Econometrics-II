# Packes required for subsequent analysis. P_load ensures these will be installed and loaded.
if (!require("pacman")) install.packages("pacman")
pacman::p_load(plm,
Formula,
car,
clubSandwich,
lmtest,
foreign)
pacman::p_load(plm,
Formula,
car,
clubSandwich,
lmtest,
foreign)
get_n_givenMDE <- function(Model_exp, sGroup, fAlpha, fPower, MDE, p, Sigma2=1){
# get t values
t_alpha <- qnorm(1-fAlpha/2, 0,Sigma2)
t_q <-qnorm(1-fPower, 0,Sigma2)
# get variance of residuals
z <- (1/sqrt(p*(1-p)))*(1/(t_alpha - t_q))
n <- Sigma2/(z*MDE)^2
return(n)
}
n_low <- get_n_givenMDE(LM_all_low, "bonus500", alpha, power, 0.1, 0.5)
n_low
n_low <- get_n_givenMDE(LM_all_low, "bonus500", alpha, power, 0.1, 0.5)
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
n_low
n_now <- nrow(LM_all_low$model)
LM_all_low <- lm(pass ~ bonus500 + math + fyeduc + p0 + effort + job, data=dfBonus %>% filter(bonus1500 != 1))
n_low
# Packes required for subsequent analysis. P_load ensures these will be installed and loaded.
if (!require("pacman")) install.packages("pacman")
pacman::p_load(plm,
Formula,
car,
clubSandwich,
lmtest,
foreign,
tidyverse)
# create df with treatment effects
dfTreatment <- data.frame(N_treated = c(100, 75, 25), N_control = c(100, 25, 75), Avg_Outcome_treated = c(9, 13,10), Avg_Outcome_control = c(7,8,9))
row.names(dfTreatment) <- c("Purple", "Blue", "Green")
# average treatment effect per group
ATE_group <- dfTreatment$Avg_Outcome_treated - dfTreatment$Avg_Outcome_control
ATE_treated <- sum(dfTreatment$N_treated/sum(dfTreatment$N_treated) * dfTreatment$Avg_Outcome_treated)
ATE_control <- sum(dfTreatment$N_control/sum(dfTreatment$N_control) * dfTreatment$Avg_Outcome_control)
ATE_treated - ATE_control
dfBonus <- read.dta("Data/bonus.dta")
dfBonus$category <- ifelse(dfBonus$bonus500 == 1, "Low-reward", ifelse(dfBonus$bonus1500 == 1, "High-reward", "Control"))
dfBonus %>%
group_by(category) %>%
summarise(perc_passed_year1 = sum(pass)/ n(),
avg_myeduc = mean(myeduc),
avg_fyeduc = mean(fyeduc),
avg_p0 = mean(p0),
math = mean(math[!is.na(math)]),
perc_job = sum(job[!is.na(job)])/n(),
avg_effort = mean(effort[!is.na(effort)]),
total = n())
LPM_simple <- lm(pass ~ category, data=dfBonus)
LPM_addedRegressors <- lm(pass ~ category + math + fyeduc + p0, data=dfBonus)
LPM_allRegressors <- lm(pass ~ category + math + fyeduc + p0 + effort + job, data=dfBonus)
mX <- dfBonus %>% select(math, fyeduc, p0, effort, job) %>% as.matrix() %>% na.omit()
cor(mX)
vif(LPM_allRegressors)
summary(LPM_simple)
summary(LPM_addedRegressors)
summary(LPM_allRegressors)
LPM_drop <- lm(dropout ~ category + math + fyeduc + p0 + effort + job, data=dfBonus)
LM_pointsYear1 <- lm(stp2001 ~ category + math + fyeduc + p0 + effort + job, data=dfBonus)
LM_pointsYear3 <- lm(stp2004 ~ category + math + fyeduc + p0 + effort + job, data=dfBonus)
summary(LPM_drop)
summary(LM_pointsYear1)
summary(LM_pointsYear3)
# define power and alpha
power <- 0.8
alpha <- 0.05
# define model for only low, and only high
qnorm(1-alpha/2, 0,1)
dfBonus$bonus500
LM_all_low <- lm(pass ~ bonus500 + math + fyeduc + p0 + effort + job, data=dfBonus %>% filter(bonus1500 != 1))
LM_all_high <- lm(pass ~ bonus1500 + math + fyeduc + p0 + effort + job, data=dfBonus %>% filter(bonus500 != 1))
get_MDE <- function(Model_exp, sGroup, fAlpha,fPower){
# get data used in the model
dfExp <- Model_exp$model
# get the n for the model
n <- nrow(dfExp)
# count how many in treatment
nTreatment <- sum(dfExp[, sGroup])
# get proportion of control
p <- (n-nTreatment)/n
# get degrees of freedom
iDF <- length(Model_exp$coefficients)
# get appropriate t-values
t_alpha <- qt(1-fAlpha/2, df = iDF)
t_q <- qt(1-fPower, df = iDF)
# get variance of residuals
Sigma2 <- var(Model_exp$residuals)
# get the MDE
MDE <- (t_alpha - t_q) * sqrt(1/(p*(1-p)))* sqrt(Sigma2/n)
return(MDE)
}
MDE_low <- get_MDE(LM_all_low, "bonus500", alpha, power)
MDE_high <- get_MDE(LM_all_high, "bonus1500", alpha, power)
get_n_givenMDE <- function(Model_exp, sGroup, fAlpha, fPower, MDE, p, Sigma2=1){
# get t values
t_alpha <- qnorm(1-fAlpha/2, 0,Sigma2)
t_q <-qnorm(1-fPower, 0,Sigma2)
# get variance of residuals
z <- (1/sqrt(p*(1-p)))*(1/(t_alpha - t_q))
n <- Sigma2/(z*MDE)^2
return(n)
}
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
n_low
get_n_givenMDE <- function(Model_exp, sGroup, fAlpha, fPower, MDE, p){
Sigma2 <- var(Model_exp$residuals)
# get t values
t_alpha <- qnorm(1-fAlpha/2, 0,Sigma2)
t_q <-qnorm(1-fPower, 0,Sigma2)
# get variance of residuals
z <- (1/sqrt(p*(1-p)))*(1/(t_alpha - t_q))
n <- Sigma2/(z*MDE)^2
return(n)
}
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
n_low
get_n_givenMDE <- function(Model_exp, sGroup, fAlpha, fPower, MDE, p){
Sigma2 <- var(Model_exp$residuals)
print(Sigma2)
# get t values
t_alpha <- qnorm(1-fAlpha/2, 0,Sigma2)
t_q <-qnorm(1-fPower, 0,Sigma2)
# get variance of residuals
z <- (1/sqrt(p*(1-p)))*(1/(t_alpha - t_q))
n <- Sigma2/(z*MDE)^2
return(n)
}
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
get_n_givenMDE <- function(Model_exp, sGroup, fAlpha, fPower, MDE, p, Sigma2 = 1){
# get t values
t_alpha <- qnorm(1-fAlpha/2, 0,Sigma2)
t_q <-qnorm(1-fPower, 0,Sigma2)
# get variance of residuals
z <- (1/sqrt(p*(1-p)))*(1/(t_alpha - t_q))
n <- Sigma2/(z*MDE)^2
return(n)
}
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
n_low
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.1, 0.8, 0.1, 0.5)
n_low
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.5, 0.1, 0.5)
n_low
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
n_low
n_low <- get_n_givenMDE(LM_all_low, "bonus500", 0.05, 0.8, 0.1, 0.5)
n_low
all.equal(as.vector(test$v), as.vector(my$v))
library(PMA)
library(tidyverse)
library(gridExtra)
### FH: This makes the code not usable if I don't have these libraries - I would need to install them first. You can change this as follows:
### if (!require("pacman")) install.packages("pacman")
###  pacman::p_load(package1, package2, package_n)
#######################################################################
# DATA PREP
#######################################################################
# load in data
load("FIFA2017_NL.RData")
# tidy data
fifa$Position <- fct_recode(fifa$Position, Goalkeeper = "Gk",
Defender = "Def", Midfielder = "Mid",
Forward = "FW")
# drop name column
X <- fifa %>% select(!c("name", "club", "Position", "eur_value",
"eur_wage", "eur_release_clause"))
# bring into matrix form, exclude intercept
X <- model.matrix(~. -1, X)
# normalise data
X <- scale(X)
#######################################################################
# FUNCTIONS
#######################################################################
# soft thresholding function
soft_thresh <- function(x, lambda) {
sign(x) * (pmax(abs(x) - lambda, 0))
}
# soft thresholding function divided by L2 norm
soft_l2norm <- function(x, lambda) {
x <- soft_thresh(x = x, lambda = lambda)
x_l2 <- sqrt(sum(x^2))
if (abs(x_l2) > .Machine$double.eps^0.5) {
x <- x / x_l2
}
x
}
# binary search function to determine optimal lambda
binary_search <- function(x, c, maxit = 100) {
## Check if x already meets the condition, or whether c is zero
x_l1 <- sum(abs(x))
if (x_l1 <= c || c == 0) {
return(0)
}
## Initialize search boundaries and iteration counter
lo <- 0
hi <- max(abs(x))
i <- 0
## Iterate
while(i < maxit) {
## Increase iteration count
i <- i + 1
## Get mean of current bounds, and evaluate L1 norm there
m <- lo + (hi - lo)/2
m_l1 <- sum(abs(soft_l2norm(x, m)))
## Update either lower or upper threshold
if (m_l1 <= c) {
hi <- m
} else {
lo <- m
}
## Break if lo and hi have converged
if (abs(lo - hi) <= sqrt(.Machine$double.eps)) {
break
}
}
return(lo + (hi - lo)/2)
}
# penalised rank one SVD
penSVD <- function(X, c1, c2, imax = 100){
# initialise parameters
v <- svd(X, nu = 0, nv = 1)$v
i <- 0
cat("Finding penalised rank-1 SVD over", imax, "steps.\n")
while (i < imax) {
i <- i + 1
if (i %% 5 == 0){
cat("Iteration", i, "\n")
}
# compute X times v
Xv <- X %*% v
# binary search for smallest lambda1 s.t. l1norm of u <= c1
lambda1 <- binary_search(Xv, c1, maxit = 100)
# update u
u <- soft_l2norm(Xv, lambda1)
# compute X'u
Xu <- t(X)  %*% u
# binary search for smallest lambda2 s.t. l1norm of v <= c2
lambda2 <- binary_search(Xu, c2, maxit = 100)
# update v
v <- soft_l2norm(Xu, lambda2)
}
cat("Algorithm converged.\n")
return(list(u = u, v = v, sigma = t(u) %*% X %*% v))
}
# compare results
# unpenalised, so just SVD
test <- PMD(X, sumabsu = sqrt(nrow(X)), sumabsv = sqrt(ncol(X)), center = FALSE)
# drop name column
X <- fifa %>% select(!c("name", "club", "Position", "eur_value",
"eur_wage", "eur_release_clause"))
# tidy data
fifa$Position <- fct_recode(fifa$Position, Goalkeeper = "Gk",
Defender = "Def", Midfielder = "Mid",
Forward = "FW")
# load in data
load("FIFA2017_NL.RData")
getwd()
setwd("C:/Users/flori/OneDrive/Documents/Tinbergen/Courses/USML")
# load in data
load("Data/FIFA2017_NL.RData")
fifa$Position <- fct_recode(fifa$Position, Goalkeeper = "Gk",
Defender = "Def", Midfielder = "Mid",
Forward = "FW")
# drop name column
X <- fifa %>% select(!c("name", "club", "Position", "eur_value",
"eur_wage", "eur_release_clause"))
# bring into matrix form, exclude intercept
X <- model.matrix(~. -1, X)
# normalise data
X <- scale(X)
#######################################################################
# FUNCTIONS
#######################################################################
# soft thresholding function
soft_thresh <- function(x, lambda) {
sign(x) * (pmax(abs(x) - lambda, 0))
}
# soft thresholding function divided by L2 norm
soft_l2norm <- function(x, lambda) {
x <- soft_thresh(x = x, lambda = lambda)
x_l2 <- sqrt(sum(x^2))
if (abs(x_l2) > .Machine$double.eps^0.5) {
x <- x / x_l2
}
x
}
# binary search function to determine optimal lambda
binary_search <- function(x, c, maxit = 100) {
## Check if x already meets the condition, or whether c is zero
x_l1 <- sum(abs(x))
if (x_l1 <= c || c == 0) {
return(0)
}
## Initialize search boundaries and iteration counter
lo <- 0
hi <- max(abs(x))
i <- 0
## Iterate
while(i < maxit) {
## Increase iteration count
i <- i + 1
## Get mean of current bounds, and evaluate L1 norm there
m <- lo + (hi - lo)/2
m_l1 <- sum(abs(soft_l2norm(x, m)))
## Update either lower or upper threshold
if (m_l1 <= c) {
hi <- m
} else {
lo <- m
}
## Break if lo and hi have converged
if (abs(lo - hi) <= sqrt(.Machine$double.eps)) {
break
}
}
return(lo + (hi - lo)/2)
}
# penalised rank one SVD
penSVD <- function(X, c1, c2, imax = 100){
# initialise parameters
v <- svd(X, nu = 0, nv = 1)$v
i <- 0
cat("Finding penalised rank-1 SVD over", imax, "steps.\n")
while (i < imax) {
i <- i + 1
if (i %% 5 == 0){
cat("Iteration", i, "\n")
}
# compute X times v
Xv <- X %*% v
# binary search for smallest lambda1 s.t. l1norm of u <= c1
lambda1 <- binary_search(Xv, c1, maxit = 100)
# update u
u <- soft_l2norm(Xv, lambda1)
# compute X'u
Xu <- t(X)  %*% u
# binary search for smallest lambda2 s.t. l1norm of v <= c2
lambda2 <- binary_search(Xu, c2, maxit = 100)
# update v
v <- soft_l2norm(Xu, lambda2)
}
cat("Algorithm converged.\n")
return(list(u = u, v = v, sigma = t(u) %*% X %*% v))
}
# compare results
# unpenalised, so just SVD
test <- PMD(X, sumabsu = sqrt(nrow(X)), sumabsv = sqrt(ncol(X)), center = FALSE)
test$v
my <- penSVD(X, c1 = sqrt(nrow(X)), c2 = sqrt(ncol(X)))
my$v
all.equal(as.vector(test$v), as.vector(my$v))
# sparse function
sparserankonePCA <- function(X, c2, imax, verbose = TRUE){
# initialise parameters
v <- svd(X, nu = 0, nv = 1)$v
i <- 0
while (i < imax) {
i <- i + 1
if ((i %% 5 == 0) & (isTRUE(verbose))){
cat("Iteration", i, "\n")
}
# compute X times v
Xv <- X %*% v
# update u
u <- Xv / sqrt(sum(Xv^2))
# compute X'u
Xu <- t(X) %*% u
# binary search for smallest lambda2 s.t. l1norm of v <= c2
lambda2 <- binary_search(Xu, c2, maxit = 100)
# update v
v <- soft_l2norm(Xu, lambda2)
}
return(list(u = u, v = v, sigma = as.numeric(t(u) %*% X %*% v)))
}
# sparse PCA function
sparsePCA <- function(X, K, c2, imax){
# check that c2 is correctly specified
if (c2 < 1 | c2 > sqrt(ncol(X))){
stop("c2 not within feasible interval (1 <= c2 <= sqrt(p)")
}
# initialise parameters
R <- X
SIGMA <- matrix(NA, nrow = K, ncol = 1)
U <- matrix(NA, nrow = nrow(X), ncol = K)
V <- matrix(NA, nrow = ncol(X), ncol = K)
cat("Initialisation finished.\n")
for (k in 1:K) {
cat("Rank-1 PCA - Iteration ", k, "\n")
# get sparse rank one PCA
results <- sparserankonePCA(R, c2 = c2, imax = imax, verbose = FALSE)
# save rank one results
SIGMA[k,1] <- results$sigma
U[,k] <- results$u
V[,k] <- results$v
# update R matrix
R <- R - (as.numeric(results$sigma) * as.matrix(results$u) %*% t(as.matrix(results$v)))
}
return(list(u = U, v = V, sigma = SIGMA))
}
# compare penalised SVD results
test <- SPC(X, sumabsv = 3, K = 2, center = FALSE, trace = FALSE)
my <- sparsePCA(X, K = 2, c2 = 3, imax =1000)
test$u[1:10,]
my$u[1:10,]
test$v[1:10,]
my$v[1:10,]
all.equal(as.vector(test$v), as.vector(my$v))
all.equal(as.vector(abs(test$u)), as.vector(abs(my$u)))
# compare unpenalised results, equivalent to ordinary SVD
test <- SPC(X, sumabsv = sqrt(ncol(X)), K = ncol(X), center = FALSE, trace = FALSE)
my <- sparsePCA(X, K = ncol(X), c2 = sqrt(ncol(X)), imax =1000)
# check near equivalence
all.equal(as.vector(abs(test$u)), as.vector(abs(my$u)))
all.equal(as.vector(abs(test$v)), as.vector(abs(my$v)))
all.equal(as.vector(test$d), as.vector(my$sigma))
as.vector(abs(test$u))
as.vector(abs(my$u))
# plot using own function and scarce SVD
# do PCA with own function
results <- sparsePCA(X, K = 6, c2 = 4, imax = 1000)
# compute principal components
PC1 <- as.data.frame(X %*% results$v[,1])
PC2 <- as.data.frame(X %*% results$v[,2])
# generate dataframe for plotting
plotdf_own <- as.data.frame(cbind(fifa$Position, PC1, PC2))
names(plotdf_own) <- c("Position", "pc1", "pc2")
# construct plot
ownplot <- ggplot(aes(x = pc1, y = pc2, color = Position), data = plotdf_own) +
geom_point() + xlab("First Principal Component") +
ylab("Second Principal Component") + ggtitle("Own Implementation") + theme_bw()
ownplot
#######################################################################
# MAIN ANALYSIS AND RESULTS
#######################################################################
# first plot scree plot to determine number of factors to retain
fullpca <- prcomp(X, center = FALSE)
# check prop of variance explained
summary(fullpca)
# scree plot
pdf("screeplot.pdf")
scree <- plot(fullpca, type = "l", main = "")
dev.off()
# for 2 factors find the optimal c2
set.seed(1)
CVresults <- SPC.cv(X, sumabsvs = seq(1, sqrt(ncol(X)), length = 60),
nfolds = 10, center = FALSE)
c2 <- CVresults$bestsumabsv1se
# conduct sparse PCA
sparsepca <- SPC(X, sumabsv = 4.5, K = 2, center = FALSE)
# format factor loadings
fl <- sparsepca$v
rownames(fl) <- colnames(X)
colnames(fl) <- c("PC1", "PC2")
fl <- as.data.frame(fl)
# output first loadings
fl1 <- fl %>% arrange(desc(abs(PC1)))
fl1 <- fl1 %>% select(PC1)
xtable(fl1)
# output second loadings
fl2 <- fl %>% arrange(desc(abs(PC2)))
fl2 <- fl2 %>% select(PC2)
xtable(fl2)
# plots
# compute principal components
PC1 <- as.data.frame(X %*% sparsepca$v[,1])
PC2 <- as.data.frame(X %*% sparsepca$v[,2])
# generate dataframe for plotting
plotdf <- as.data.frame(cbind(fifa$Position, PC1, PC2))
names(plotdf) <- c("Position", "pc1", "pc2")
# construct plot
resultsplot <- ggplot(aes(x = pc1, y = pc2, color = Position), data = plotdf) +
geom_point() + xlab("First Principal Component") +
ylab("Second Principal Component") + theme_bw() + theme(legend.position = "top")
resultsplot
# sparse function
sparserankonePCA <- function(X, c2, imax, verbose = TRUE){
### When does it converge?
# initialise parameters
v <- svd(X, nu = 0, nv = 1)$v
i <- 0
while (i < imax) {
i <- i + 1
## tell number of iterations
if ((i %% 5 == 0) & (isTRUE(verbose))){
cat("Iteration", i, "\n")
}
# compute X times v
Xv <- X %*% v
# update u
u <- Xv / sqrt(sum(Xv^2))
# compute X'u
Xu <- t(X) %*% u
# binary search for smallest lambda2 s.t. l1norm of v <= c2
lambda2 <- binary_search(Xu, c2, maxit = 100)
# update v
v <- soft_l2norm(Xu, lambda2)
}
return(list(u = u, v = v, sigma = as.numeric(t(u) %*% X %*% v)))
}
my <- sparsePCA(X, K = 2, c2 = 3, imax =1000)
